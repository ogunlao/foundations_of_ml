{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "by Marc Deisenroth and Yicheng Luo\n",
    "\n",
    "We will implement the PCA algorithm using the projection perspective. We will first implement PCA, then apply it to the MNIST digit dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "1. Write code that implements PCA.\n",
    "2. Write code that implements PCA for high-dimensional datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first import the packages we need for this week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PACKAGE: DO NOT EDIT THIS CELL\n",
    "import numpy as np\n",
    "import timeit\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "from ipywidgets import interact\n",
    "from sklearn.datasets import fetch_mldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PACKAGE: DO NOT EDIT THIS CELL\n",
    "from sklearn.datasets import fetch_openml\n",
    "images, labels = fetch_openml('mnist_784', version=1, return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot a digit from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(images[0].reshape(28,28), cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we implement PCA, we will need to do some data preprocessing. In this assessment, some of them \n",
    "will be implemented by you, others we will take care of. However, when you are working on real world problems, you will need to do all these steps by yourself.\n",
    "\n",
    "The preprocessing steps we will do are\n",
    "1. Convert unsigned interger 8 (uint8) encoding of pixels to a floating point number between 0 and 1.\n",
    "2. Subtract from each image the mean $\\boldsymbol \\mu$.\n",
    "3. Scale each dimension of each image by $\\frac{1}{\\sigma}$ where $\\sigma$ is the stardard deviation.\n",
    "\n",
    "The steps above ensure that our images will have zero mean and one variance. These preprocessing\n",
    "steps are also known as [Data Normalization or Feature Scaling](https://en.wikipedia.org/wiki/Feature_scaling)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PCA\n",
    "\n",
    "Now we will implement PCA. Before we do that, let's pause for a moment and\n",
    "think about the steps for performing PCA. Assume that we are performing PCA on\n",
    "some dataset $\\boldsymbol X$ for $M$ principal components. \n",
    "We then need to perform the following steps, which we break into parts:\n",
    "\n",
    "1. Data normalization (`normalize`).\n",
    "2. Find eigenvalues and corresponding eigenvectors for the covariance matrix $S$.\n",
    "   Sort by the largest eigenvalues and the corresponding eigenvectors (`eig`).\n",
    "\n",
    "After these steps, we can then compute the projection and reconstruction of the data onto the spaced spanned by the top $n$ eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Sebastian Book Code](https://sebastianraschka.com/Articles/2015_pca_in_3_steps.html)\n",
    "\n",
    "1. [Stack Overflow - Explanation](https://stats.stackexchange.com/questions/38835/clear-description-of-pca-using-svd-of-covariance-matrix?rq=1)  \n",
    "1. [Stack Overflow - Explanation](https://stats.stackexchange.com/questions/229092/how-to-reverse-pca-and-reconstruct-original-variables-from-several-principal-com)\n",
    "1. [Anothe code](https://glowingpython.blogspot.com/2011/07/pca-and-image-compression-with-numpy.html)  \n",
    "Code not giving correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    \"\"\"Normalize the given dataset X\n",
    "    Args:\n",
    "        X: ndarray, dataset\n",
    "    \n",
    "    Returns:\n",
    "        (Xbar, mean, std): tuple of ndarray, Xbar is the normalized dataset\n",
    "        with mean 0 and standard deviation 1; mean and std are the \n",
    "        mean and standard deviation respectively.\n",
    "    \n",
    "    Note:\n",
    "        You will encounter dimensions where the standard deviation is\n",
    "        zero, for those when you do normalization the normalized data\n",
    "        will be NaN. Handle this by setting using `std = 1` for those \n",
    "        dimensions when doing normalization.\n",
    "    \"\"\"\n",
    "    mu = np.mean(X, axis=0) # <-- EDIT THIS, compute the mean of X\n",
    "    std = np.std(X, axis=0)\n",
    "    std_filled = std.copy()\n",
    "    std_filled[std==0] = 1\n",
    "    Xbar = (X - mu)/std_filled  # <-- EDIT THIS, compute the normalized data Xbar\n",
    "    return Xbar, mu, std\n",
    "\n",
    "def eig(S):\n",
    "    \"\"\"Compute the eigenvalues and corresponding eigenvectors \n",
    "        for the covariance matrix S.\n",
    "    Args:\n",
    "        S: ndarray, covariance matrix\n",
    "    \n",
    "    Returns:\n",
    "        (eigvals, eigvecs): ndarray, the eigenvalues and eigenvectors\n",
    "\n",
    "    Note:\n",
    "        the eigenvals and eigenvecs should be sorted in descending\n",
    "        order of the eigen values\n",
    "    \"\"\"\n",
    "    eig_vals, eig_vecs = np.linalg.eig(S)\n",
    "    eigval_sorted_idx = np.argsort(eig_vals) # Index of sorted array \n",
    "    eigval_sorted_idx_max_first = np.sort(eigval_sorted_idx)[::-1] # Sorted index descending\n",
    "    eig_vals_sorted = eig_vals[[eigval_sorted_idx_max_first]]\n",
    "    eig_vecs_sorted = np.take(eig_vecs, eigval_sorted_idx_max_first, axis=1)\n",
    "    #eig_vecs_sorted = (eig_vecs.T[[eigval_sorted_idx_max_first]]).T.copy() #same as above\n",
    "    return (eig_vals_sorted, eig_vecs_sorted) # <-- EDIT THIS to return the eigenvalues and corresponding eigenvectors\n",
    "\n",
    "def projection_matrix(B, num_components):\n",
    "    \"\"\"Compute the projection matrix onto the space spanned by `B`\n",
    "    Args:\n",
    "        B: ndarray of dimension (D, M), the basis for the subspace\n",
    "    \n",
    "    Returns:\n",
    "        P: the projection matrix , a matrix of our concatenated top k eigenvectors.\n",
    "    \"\"\"\n",
    "    if num_components  <= B.shape[-1]: #Cannot have num_components greater than size of eigenvectors\n",
    "        P = B[:, :num_components]\n",
    "    else: P = B\n",
    "\n",
    "    return P # <-- EDIT THIS to compute the projection matrix\n",
    "\n",
    "def PCA(X, num_components):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X: ndarray of size (N, D), where D is the dimension of the data,\n",
    "           and N is the number of datapoints\n",
    "        num_components: the number of principal components to use.\n",
    "    Returns:\n",
    "        X_reconstruct: ndarray of the reconstruction\n",
    "        of X from the first `num_components` principal components.\n",
    "    \"\"\" \n",
    "    cov_mat = np.cov(X.T) # Compute the covariance matrix (D,D)\n",
    "    eig_val, eig_vec = eig(cov_mat) # Compute the eigval (D,) & eigvec (D,D) with sorted components\n",
    "    P = projection_matrix(eig_vec, num_components) # (D, num_components )\n",
    "    \n",
    "    X_compressed = X@P # (N, num_components) \n",
    "    X_reconstruct = X_compressed @ P.T # (N, D)\n",
    "    #X = (eig_vec@np.diag(eig_val)[:, :num_components])@np.linalg.inv(eig_vec)\n",
    "    \n",
    "    # your solution should take advantage of the functions you have implemented above.\n",
    "    return X_reconstruct # <--EDIT THIS to return the reconstruction of X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some preprocessing of the data\n",
    "NUM_DATAPOINTS = 1000\n",
    "X = (images.reshape(-1, 28 * 28)[:NUM_DATAPOINTS])/ 255.\n",
    "Xbar, mu, std = normalize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_component in range(5, 20):\n",
    "    from sklearn.decomposition import PCA as SKPCA\n",
    "    # We can compute a standard solution given by scikit-learn's implementation of PCA\n",
    "    pca = SKPCA(n_components=num_component, svd_solver='full')\n",
    "    sklearn_reconst = pca.inverse_transform(pca.fit_transform(Xbar))\n",
    "    reconst = PCA(Xbar, num_component)\n",
    "    np.testing.assert_almost_equal(reconst, sklearn_reconst)\n",
    "    print(np.square(reconst - sklearn_reconst).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The greater number of of principal components we use, the smaller will our reconstruction\n",
    "error be. Now, let's answer the following question: \n",
    "\n",
    "\n",
    "> How many principal components do we need\n",
    "> in order to reach a Mean Squared Error (MSE) of less than $100$ for our dataset?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have provided a function in the next cell that computes the mean squared error (MSE), which will be useful for answering the question above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(predict, actual):\n",
    "    \"\"\"Helper function for computing the mean squared error (MSE)\"\"\"\n",
    "    return np.square(predict - actual).sum(axis=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = []\n",
    "reconstructions = []\n",
    "# iterate over different numbers of principal components, and compute the MSE\n",
    "for num_component in range(1, 100):\n",
    "    reconst = PCA(Xbar, num_component)\n",
    "    error = mse(reconst, Xbar)\n",
    "    reconstructions.append(reconst)\n",
    "    # print('n = {:d}, reconstruction_error = {:f}'.format(num_component, error))\n",
    "    loss.append((num_component, error))\n",
    "\n",
    "reconstructions = np.asarray(reconstructions)\n",
    "reconstructions = reconstructions * std + mu # \"unnormalize\" the reconstructed image\n",
    "loss = np.asarray(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# create a table showing the number of principal components and MSE\n",
    "pd.DataFrame(loss).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also put these numbers into perspective by plotting them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(loss[:,0], loss[:,1]);\n",
    "ax.axhline(100, linestyle='--', color='r', linewidth=2)\n",
    "ax.xaxis.set_ticks(np.arange(1, 100, 5));\n",
    "ax.set(xlabel='num_components', ylabel='MSE', title='MSE vs number of principal components');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But _numbers dont't tell us everything_! Just what does it mean _qualitatively_ for the loss to decrease from around\n",
    "$450.0$ to less than $100.0$?\n",
    "\n",
    "Let's find out! In the next cell, we draw the the leftmost image is the original dight. Then we show the reconstruction of the image on the right, in descending number of principal components used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@interact(image_idx=(0, 1000))\n",
    "def show_num_components_reconst(image_idx):\n",
    "    fig, ax = plt.subplots(figsize=(20., 20.))\n",
    "    actual = X[image_idx]\n",
    "    # concatenate the actual and reconstructed images as large image before plotting it\n",
    "    x = np.concatenate([actual[np.newaxis, :], reconstructions[:, image_idx]])\n",
    "    ax.imshow(np.hstack(x.reshape(-1, 28, 28)[np.arange(10)]),\n",
    "              cmap='gray');\n",
    "    ax.axvline(28, color='orange', linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also browse throught the reconstructions for other digits. Once again, `interact` becomes handy for visualing the reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@interact(i=(0, 10))\n",
    "def show_pca_digits(i=1):\n",
    "    \"\"\"Show the i th digit and its reconstruction\"\"\"\n",
    "    plt.figure(figsize=(4,4))\n",
    "    actual_sample = X[i].reshape(28,28)\n",
    "    reconst_sample = (reconst[i, :] * std + mu).reshape(28, 28)\n",
    "    plt.imshow(np.hstack([actual_sample, reconst_sample]), cmap='gray')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "mathematics-machine-learning-pca",
   "graded_item_id": "CXC11",
   "launcher_item_id": "ub5A7"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression using Newton Method and KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa', 'versicolor', 'virginica']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "dataset = load_iris()\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "\n",
    "target_names = list(dataset.target_names)\n",
    "print(target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change to binary class\n",
    "y = (y > 0).astype(int)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: Linear Regression\n",
    "class LogReg:\n",
    "    \"\"\"\n",
    "    This implementation of Logistic Regression uses the Newton's Method for optimization.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_iters=20, tolerance = 1e-10, epsilon = 10e-8, threshold=0.5, verbose=False):\n",
    "        self.num_iters = num_iters\n",
    "        self.tolerance = tolerance\n",
    "        self.epsilon = epsilon # subtracted to make hessian invertible\n",
    "        self.threshold = threshold\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def add_ones(self, X):\n",
    "        return np.concatenate((np.ones((len(X),1)), X), axis = 1)\n",
    "    \n",
    "    \n",
    "    def sigmoid(self, X, theta):\n",
    "        return 1/(1 + np.exp(X@theta))\n",
    "    \n",
    "    def get_hessian_inv(self, X, theta):\n",
    "        htheta1theta = np.diag(self.sigmoid(X, theta).T@(1-self.sigmoid(X, theta))).reshape(-1, 1)\n",
    "        hessian = (X.T*htheta1theta)@X\n",
    "        hessian_inv = np.linalg.inv(hessian + self.epsilon*np.eye(X.T.shape[0]))\n",
    "        return hessian_inv\n",
    "    \n",
    "    def cost(self, X, y_true):\n",
    "        return np.mean((X@self.theta - y_true)**2)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X = X.copy()\n",
    "        X = self.add_ones(X)\n",
    "        y = y.reshape(-1, 1)\n",
    "        \n",
    "        self.theta = np.zeros((len(X[0]), 1))\n",
    "        \n",
    "        current_iter = 1\n",
    "        norm = 1\n",
    "        while (norm >= self.tolerance and current_iter < self.num_iters):\n",
    "            old_theta = self.theta.copy()\n",
    "            grad = -X.T@(y - self.sigmoid(X, self.theta))\n",
    "            grad= grad.reshape(-1, 1)\n",
    "            hessian_inv = self.get_hessian_inv(X, self.theta)\n",
    "            \n",
    "            self.theta = self.theta - hessian_inv@grad\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f'cost for {current_iter} iteration : {self.cost(X, y)}')\n",
    "            norm = np.linalg.norm(old_theta - self.theta)\n",
    "            current_iter += 1\n",
    "            \n",
    "        return self.theta\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        \"\"\"\n",
    "        Returns mse loss for a dataset evaluated on the hypothesis\n",
    "        \"\"\"\n",
    "        X = self.add_ones(X)\n",
    "        return self.cost(X, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        prob = self.predict_proba(X)\n",
    "        return (prob > self.threshold).astype(int)\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Returns probability of predictions.\n",
    "        \"\"\"\n",
    "        X = self.add_ones(X)  \n",
    "        return self.sigmoid(X, self.theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogReg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.20655598],\n",
       "       [-0.03572809],\n",
       "       [-0.13137099],\n",
       "       [ 0.12153157],\n",
       "       [ 0.03110443]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = logreg.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66.66666666666667"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y == predictions) / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KFoldCrossVal:\n",
    "    \"\"\"\n",
    "    Performs k-fold cross validation on each combination of hyperparameter set\n",
    "    \n",
    "    Input\n",
    "    ............\n",
    "    X : Features (m, n)\n",
    "    y : target (m, 1)\n",
    "    hyperparameter_Set : Dictionary of hyperparameters for k-fold\n",
    "    num_of_folds: Number of folds, k; default:10\n",
    "    verbose: Checks whether to print parameters on every iteration; Boolean; Default: False\n",
    "    \"\"\"\n",
    "    def __init__(self, hyperparameter_set, num_of_folds=10, verbose=True):\n",
    "        self.hyperparameter_set = hyperparameter_set\n",
    "        self.k = num_of_folds\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    import sys\n",
    "    if ('numpy' not in sys.modules) and ('np' not in dir()): #import numpy if not done\n",
    "        import numpy as np\n",
    "    \n",
    "    #def get_model_no(self):\n",
    "        \n",
    "        \n",
    "    def shuffle_data(self, X, y):\n",
    "        shuffle_arr = np.random.permutation(len(X))\n",
    "        X_shuffled = X[shuffle_arr]\n",
    "        y_shuffled = y[shuffle_arr].reshape(-1, 1)\n",
    "        \n",
    "        return X_shuffled, y_shuffled\n",
    "    \n",
    "    def get_kfold_arr_index(self, subset_size, last_index):\n",
    "        array_indexes = [0]\n",
    "        for fold_no in range(self.k):\n",
    "            if fold_no != (self.k-1):\n",
    "                array_indexes.append((fold_no+1)*subset_size)\n",
    "            elif fold_no == (self.k - 1): # To accomodate examples not part of the \n",
    "                array_indexes.append(last_index) #for last index\n",
    "        return array_indexes\n",
    "    \n",
    "    def get_split_data_fold(self, X, y, array_indexes, fold_no):\n",
    "        start = array_indexes[fold_no]\n",
    "        end = array_indexes[fold_no+1]\n",
    "        X_val = X[start: end]\n",
    "        y_val = y[start: end]\n",
    "        \n",
    "        X_train = np.delete(X, [start,end], axis=0)\n",
    "        y_train = np.delete(y, [start,end]).reshape(-1,1)\n",
    "        \n",
    "        return X_train, y_train, X_val, y_val\n",
    "    \n",
    "    def get_hyperparameter_sets(self, hyperparameter_dict):   \n",
    "        \"\"\"\n",
    "        Converts the hyperparameter dictionary into all possible combinations of hyperparameters\n",
    "\n",
    "        Return\n",
    "        ..............\n",
    "        Array of hyperparameter set\n",
    "        \"\"\"\n",
    "        import itertools\n",
    "\n",
    "        parameter_keys = hyperparameter_dict.keys()\n",
    "        parameter_values = hyperparameter_dict.values()\n",
    "\n",
    "        parameter_array = []\n",
    "        for params in itertools.product(*parameter_values):\n",
    "            parameter_array.append(params)\n",
    "\n",
    "        parameter_sets = []\n",
    "        for parameter_values in parameter_array:\n",
    "            parameter_set = dict(zip(parameter_keys, parameter_values))\n",
    "            parameter_sets.append(parameter_set)\n",
    "\n",
    "        return parameter_sets\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        models = self.get_hyperparameter_sets(self.hyperparameter_set)\n",
    "        \n",
    "        #print(f'Performing k-fold for {len(models)} models and {len(models) * self.k} cross validations' )\n",
    "        m = len(X)\n",
    "        \n",
    "        generalization_mse = []\n",
    "        X, y = self.shuffle_data(X, y)\n",
    "        subset_size = int(m/self.k)\n",
    "        \n",
    "        array_indexes = self.get_kfold_arr_index(subset_size, m+1)\n",
    "        \n",
    "        for hyperparameters in models:\n",
    "            model = LogReg(**hyperparameters)\n",
    "            fold_mse_arr = []\n",
    "            for fold_no in range(self.k - 1):\n",
    "                X_train, y_train, X_val, y_val = self.get_split_data_fold(X, y, array_indexes, fold_no)\n",
    "                model.fit(X_train, y_train)\n",
    "                mse = model.evaluate(X_val, y_val)\n",
    "                fold_mse_arr.append(mse)\n",
    "            cv_mse = np.mean(fold_mse_arr)\n",
    "            if self.verbose:\n",
    "                print(f'{hyperparameters}, mse: {cv_mse}')\n",
    "            generalization_mse.append(cv_mse)\n",
    "            \n",
    "        lowest_gen_mse_index = np.argmin(generalization_mse)\n",
    "        lowest_mse = generalization_mse[lowest_gen_mse_index]\n",
    "        best_model = models[lowest_gen_mse_index]\n",
    "        \n",
    "        return lowest_mse, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp = {\n",
    "    'num_iters': [1, 2, 3, 4],\n",
    "    'tolerance': [1e-3, 1e-5, 1e-7, 1e-10],\n",
    "    'epsilon': [10e-6, 10e-8, 10e-10],\n",
    "    'threshold': [0.45, 0.5]\n",
    "}\n",
    "\n",
    "kcv = KFoldCrossVal(hyp, 5, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6302374734075014,\n",
       " {'num_iters': 4, 'tolerance': 0.001, 'epsilon': 1e-09, 'threshold': 0.45})"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kcv.evaluate(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
